{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "878e0113-23e5-482d-ba13-0136bb0f457d",
   "metadata": {},
   "source": [
    "# Assessments across interventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4434d7-41f0-4c08-929f-b8840ab0fc91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import bigquery\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_storage\n",
    " \n",
    "#other needed libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.makedirs('../figs', exist_ok=True)\n",
    "import sys\n",
    "sys.path.append('../code')\n",
    "from data_cleaning import gender_mapping\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "color='#702A7D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0099a1f-7161-49ef-8d86-5c5af43367de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/home/jupyter/.config/gcloud/application_default_credentials.json\"\n",
    "\n",
    "#Instatiate BigQuery Client\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609e3ea4-0f2f-414c-a2ad-1e7f6049ce4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "\n",
    "WITH address_match AS (\n",
    "  SELECT \n",
    "    a.person_id,\n",
    "    a.assessmentid,\n",
    "    ah.SOA as LSOA\n",
    "  FROM \n",
    "    `yhcr-prd-bradfor-bia-core.CB_2649.tbl_bmbc_Childrens_Social_Services_Assessments` a\n",
    "  LEFT JOIN \n",
    "    `yhcr-prd-bradfor-bia-core.CB_2649.tbl_Person_Address_History` ah\n",
    "    ON a.person_id = ah.person_id\n",
    "  QUALIFY ROW_NUMBER() OVER(PARTITION BY a.person_id, a.AssessmentID \n",
    "                           ORDER BY ABS(TIMESTAMP_DIFF(a.StartDate, CAST(ah.DateEvent AS TIMESTAMP), DAY))) = 1\n",
    ")\n",
    "\n",
    "SELECT \n",
    "  a.person_id, \n",
    "  a.assessmentid, \n",
    "  f.category,\n",
    "  f.subcategory,\n",
    "  a.StartDate,\n",
    "  addr.LSOA\n",
    "FROM \n",
    "  `yhcr-prd-bradfor-bia-core.CB_2649.tbl_bmbc_Childrens_Social_Services_Assessments` a\n",
    "LEFT JOIN \n",
    "  `CB_2649.cb_FactorLookup` f\n",
    "  ON a.factorid = f.factorid\n",
    "LEFT JOIN\n",
    "  address_match addr\n",
    "  ON a.person_id = addr.person_id AND a.assessmentid = addr.AssessmentID\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172dcfce-0726-4ff7-b3ab-4b3570798c29",
   "metadata": {},
   "source": [
    "# Load Query into Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf76afb-efc8-4d1e-9d42-3e3b32da516b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pandas_gbq.read_gbq(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b2a21d-f3dd-414d-b54f-99f4b9c0e722",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b01fdcb-c4aa-49c7-a483-ed6c8df41b9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sort by date (descending) so the most recent entry comes first\n",
    "df_sorted = df.sort_values('StartDate', ascending=False)\n",
    "\n",
    "# Keep the first occurrence of each assessment ID (the most recent one)\n",
    "df_unique_assessments = df_sorted.drop_duplicates('assessmentid')\n",
    "\n",
    "# Verify we have exactly 11,218 rows\n",
    "print(f\"Rows after selecting one entry per assessment: {len(df_unique_assessments)}\")\n",
    "\n",
    "# Create a pivot table with categories\n",
    "# create a dummy column\n",
    "df['dummy'] = 1\n",
    "\n",
    "# Create a mapping from assessment ID to person_id and date\n",
    "assessment_info = df_unique_assessments[['assessmentid', 'person_id', 'StartDate', 'LSOA']]\n",
    "\n",
    "# Create the pivot table with just assessment ID and categories\n",
    "df_categories = pd.pivot_table(\n",
    "    df,\n",
    "    index=['assessmentid'],\n",
    "    columns='category',\n",
    "    values='dummy',\n",
    "    aggfunc='max',\n",
    "    fill_value=0\n",
    ").reset_index()\n",
    "\n",
    "# Merge the assessment info back to get person_id and date\n",
    "df_wide = pd.merge(\n",
    "    assessment_info,\n",
    "    df_categories,\n",
    "    on='assessmentid',\n",
    "    how='left'\n",
    ")\n",
    "# Verify we have exactly 11,218 rows\n",
    "print(f\"Final rows in wide format: {len(df_wide)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e6756-0d16-4227-b363-c0cec0462c74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate patterns per case\n",
    "reasons_per_case = df_wide.iloc[:, 4:].sum(axis=1)  # excluding ids, date, and LSOA\n",
    "\n",
    "# Calculate percentages and counts\n",
    "total_cases = len(reasons_per_case)\n",
    "single_reason_count = (reasons_per_case == 1).sum()\n",
    "multiple_reasons_count = (reasons_per_case > 1).sum()\n",
    "\n",
    "single_reason_percent = (single_reason_count / total_cases) * 100\n",
    "multiple_reasons_percent = (multiple_reasons_count / total_cases) * 100\n",
    "\n",
    "print(\"=== Entry Pattern Analysis ===\")\n",
    "print(f\"Total number of cases: {total_cases}\")\n",
    "print(f\"Cases with single reason: {single_reason_count} ({single_reason_percent:.1f}%)\")\n",
    "print(f\"Cases with multiple reasons: {multiple_reasons_count} ({multiple_reasons_percent:.1f}%)\")\n",
    "print(f\"Average number of reasons per case: {reasons_per_case.mean():.2f}\")\n",
    "\n",
    "# Further breakdown\n",
    "print(\"\\nDistribution of number of reasons:\")\n",
    "reason_distribution = reasons_per_case.value_counts().sort_index()\n",
    "for n_reasons, count in reason_distribution.items():\n",
    "    percentage = (count / total_cases) * 100\n",
    "    print(f\"{n_reasons} reason(s): {count} cases ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4680eba-3711-441d-9e02-0e917aaf4822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sum up each category column and sort in descending order\n",
    "category_counts = df_wide.iloc[:, 4:].sum().sort_values(ascending=False)\n",
    "\n",
    "# Calculate percentages\n",
    "category_percentages = (category_counts / total_cases * 100)\n",
    "\n",
    "# Combine counts and percentages in a readable format\n",
    "category_analysis = pd.DataFrame({\n",
    "    'Count': category_counts,\n",
    "    'Percentage': category_percentages\n",
    "}).round(1)  # Round percentages to 1 decimal place\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(\n",
    "    x=category_analysis['Percentage'],\n",
    "    y=category_analysis.index,\n",
    "    palette='RdYlBu'  \n",
    ")\n",
    "\n",
    "plt.title('Reasons in Assessments: Percentage of Cases', pad=20)\n",
    "plt.xlabel('Percentage of Cases (%)')\n",
    "plt.ylabel('Assessment Reason')\n",
    "# Add percentage labels on bars\n",
    "for i, v in enumerate(category_analysis['Percentage']):\n",
    "    plt.text(v, i, f'{v}%', va='center')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f'../figs/assessment_reason.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacadbd8-03a0-422d-a9e2-16d1326f5f01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find most common reason combinations\n",
    "# Create co-occurrence matrix\n",
    "co_occurrence = df_wide.iloc[:, 4:].T.dot(df_wide.iloc[:, 4:])\n",
    "\n",
    "# Find most common combinations\n",
    "combinations = []\n",
    "for i in range(len(co_occurrence)):\n",
    "    for j in range(i+1, len(co_occurrence)):\n",
    "        combinations.append({\n",
    "            'reason1': co_occurrence.index[i],\n",
    "            'reason2': co_occurrence.index[j],\n",
    "            'count': co_occurrence.iloc[i,j]\n",
    "        })\n",
    "\n",
    "combinations_df = pd.DataFrame(combinations)\n",
    "print(\"\\n=== Most Common Reason Combinations ===\")\n",
    "print(combinations_df.sort_values('count', ascending=False).head(10).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63819c73-d789-42a6-940a-1cdae09a58c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Correlation between categories\n",
    "plt.figure(figsize=(20,10))\n",
    "correlations = df_wide.iloc[:, 4:].corr()\n",
    "sns.heatmap(correlations, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation between Categories\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527cacd3-f31b-4ea8-96de-0eb52f441f4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example of unique LSOA prefixes\n",
    "df_wide['lsoa_prefix'] = df_wide['LSOA'].str[:3]\n",
    "print(df_wide['lsoa_prefix'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b31da72-985e-42be-8abc-573a749136d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter to keep only English LSOAs (starting with 'E01') and remove empty strings\n",
    "assessment_with_lsoa = df_wide[df_wide['LSOA'].str.startswith('E01', na=False)].drop(columns='lsoa_prefix')\n",
    "\n",
    "# Assessments without LSOA and empty strings\n",
    "assessments_without_lsoa = df_wide.loc[(df_wide['LSOA'].isnull()) | (df_wide['LSOA'] == '')].drop(columns='lsoa_prefix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb14ee18-ec85-46bc-87c4-0af956e17e1e",
   "metadata": {},
   "source": [
    "## Create Percentage of cases plot for assessments with and without LSOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532cd463-6a14-4898-82bb-509dd65e9596",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine the two dataframes for easy comaprison\n",
    "\n",
    "# Calculate percentages for with LSOA\n",
    "total_with = len(assessment_with_lsoa.iloc[:, 4:])\n",
    "counts_with = assessment_with_lsoa.iloc[:, 4:].sum().sort_values(ascending=False)\n",
    "percentages_with = (counts_with / total_with * 100).round(1)\n",
    "df_with = pd.DataFrame({'Percentage': percentages_with})\n",
    "\n",
    "# Calculate percentages for without LSOA\n",
    "total_without = len(assessments_without_lsoa.iloc[:, 4:])\n",
    "counts_without = assessments_without_lsoa.iloc[:, 4:].sum().sort_values(ascending=False)\n",
    "percentages_without = (counts_without / total_without * 100).round(1)\n",
    "df_without = pd.DataFrame({'Percentage': percentages_without})\n",
    "\n",
    "# Merge the data for comparison\n",
    "comparison = pd.DataFrame({\n",
    "    'With LSOA': percentages_with,\n",
    "    'Without LSOA': percentages_without\n",
    "})\n",
    "\n",
    "# Sort by the average of both columns to maintain consistent ordering\n",
    "comparison = comparison.reindex(comparison.mean(axis=1).sort_values(ascending=True).index)\n",
    "\n",
    "# Plot side-by-side\n",
    "plt.figure(figsize=(14, 10))\n",
    "comparison.plot(kind='barh', figsize=(14, 10))\n",
    "plt.title('Comparison of Assessment Reasons (% of Cases)', fontsize=16, pad=20)\n",
    "plt.xlabel('Percentage of Cases (%)', fontsize=12)\n",
    "plt.ylabel('Assessment Reason', fontsize=12)\n",
    "plt.legend(loc='center right')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add percentage labels with better positioning\n",
    "for i, (with_val, without_val) in enumerate(zip(comparison['With LSOA'], comparison['Without LSOA'])):\n",
    "    # Place labels at the end of each bar with consistent positioning\n",
    "    plt.text(with_val + 0.3, i - 0.15, f'{with_val}%', va='center', fontsize=9)\n",
    "    plt.text(without_val + 0.3, i + 0.15, f'{without_val}%', va='center', fontsize=9)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f'../figs/assessment_reason_compariosns.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92093704-6077-4dbf-9331-5f756425de28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify the top 2 categories\n",
    "top_2_categories = category_analysis.head(2).index.tolist()\n",
    "print(f\"\\nAnalyzing subcategories for: {top_2_categories}\\n\")\n",
    "\n",
    "# Create a figure for subcategory analysis\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# For each top category, analyze its subcategories\n",
    "for i, category in enumerate(top_2_categories):\n",
    "    # get all assessment IDs that have this category\n",
    "    category_assessments = df[df['category'] == category]['assessmentid'].unique()\n",
    "\n",
    "    # create a dataframe with for each assessment\n",
    "    # that has this category\n",
    "    df_category_unique = df_sorted[\n",
    "        (df_sorted['assessmentid'].isin(category_assessments)) &\n",
    "        (df_sorted['category'] == category)\n",
    "    ].drop_duplicates('assessmentid')\n",
    "\n",
    "    # Now count subcategories from this filtered dataframe\n",
    "    subcat_counts = df_category_unique['subcategory'].value_counts().drop([None], errors='ignore')\n",
    "\n",
    "    # Calculate percentages relative to TOTAL cases\n",
    "    subcat_percentages = (subcat_counts / total_cases * 100).round(1)\n",
    "\n",
    "    # Create subplot\n",
    "    ax = plt.subplot(1, 2, i+1)\n",
    "\n",
    "    # Plot \n",
    "    bars = ax.barh(y=range(len(subcat_percentages)),\n",
    "                  width=subcat_percentages.values,\n",
    "                  color='#702A7D',\n",
    "                  height=0.8)  \n",
    "\n",
    "    # Set y-tick positions and labels\n",
    "    ax.set_yticks(range(len(subcat_percentages)))\n",
    "    ax.set_yticklabels(subcat_percentages.index)\n",
    "\n",
    "    # Reduce space between bars by adjusting the y-axis limits\n",
    "    if len(subcat_percentages) > 1:\n",
    "        ax.set_ylim(-0.5, len(subcat_percentages) - 0.5)\n",
    "    # Add percentage labels\n",
    "    for j, (bar, percentage) in enumerate(zip(bars, subcat_percentages)):\n",
    "        ax.text(percentage, j, f' {percentage}%', va='center')\n",
    "    ax.set_title(f'Subcategories within {category} (% of all cases)', fontsize=10, pad=20)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f'../figs/top2_category_subcategory_plot.png', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "r-cpu.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/r-cpu:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
